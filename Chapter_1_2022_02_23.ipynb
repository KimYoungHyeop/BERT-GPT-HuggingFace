{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_1_2022_02_23.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNkQsp10O3/U5E4HrVHk/RM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 1. 처음 만나는 자연어 처리"
      ],
      "metadata": {
        "id": "H4OnggAleFzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <1-1 딥러닝 기반 자연어 처리 모델>"
      ],
      "metadata": {
        "id": "mgb0QDEyePoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 기계의 자연어 처리\n",
        "---\n",
        "기계가 사람의 말을 알아듣는 것처럼 보이게 하려면 어떤 요소들이 있어야 할까요? 우선은 ***모델(model)*** 이라는 개념부터 소개해 보겠습니다. 모델은 입력을 받아 어떤 처리를 수행하는 ***함수(function)*** 입니다.  \n",
        "\n",
        "모델의 출력은 확률이라는 점에 주목해야 합니다. ***확률(probability)*** 이란 어떤 사건이 나타날 가능성을 의미하는 수치이며 0에서 1 사이의 값으로 나타냅니다. 다시 말해 모델은 어떤 입력을 받아서 해당 입력이 특정 범주일 확률을 반환하는 ***확률 함수***입니다.  \n",
        "그렇다면 자연어 처리 모델의 입력은 무엇일까요? 사람의 말, 즉 자연어입니다."
      ],
      "metadata": {
        "id": "iG5A0o-IePqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "예를 들어 우리가 영화 리뷰의 ***감성(sentiment)*** 을 맞히는 자연어 처리 모델을 만든다고 가정해 봅시다. 그러면 우리가 만든 감성 분석 모델은 다음 수식처럼 함수 f로 써볼 수 있습니다. 이 모델은 자연어(문장)를 입력받아 복잡한 내부 계산 과정을 거쳐서 해당 문장이 긍정(positive) 일 확률, 중립(neutral)일 확률, 부정(negative)일 확률을 출력합니다.  \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(재미가 없는 편인 영화에요) = [0.0 0.3 0.7]  \n",
        "\\end{align}\n",
        "$$\n",
        "$$\n",
        "\\begin{align}\n",
        "f(단언컨대 이 영화 재미 있어요) = [1.0 0.0 0.0]\n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "cwGoErCRePs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 종류는 정말 다양합니다. 입력 특성과 목적 등에 따라 최적이라고 판단되는 걸 선택하면 됩니다.  \n",
        "그러면 요즘 가장 인기 있는 모델 종류는 무엇일까요? 바로 ***딥러닝(deep learning)*** 입니다. 기존의 다른 구조보다 성능이 월등히 좋기 때문입니다. 딥러닝이란 데이터 패턴을 스스로 익히는 인공지능의 한 갈래입니다. 여기에서 '딥(deep)'이란 많은 ***은닉층(hidden layer)*** 을 사용한다는 의미입니다. 딥러닝은 이미지 분류, 음성 인식 및 합성, 자연어 처리 등 다양한 분야에서 널리 쓰이고 있습니다. 딥러닝 가운데서도 ***BERT(Bidirectional Encoder Repersentation from Transformers)*** 나 ***GPT(Generative Pre-trained Transformer)*** 등이 주목받고 있습니다. 이들을 ***딥러닝 기반 자연어 처리 모델*** 이라고 부릅니다."
      ],
      "metadata": {
        "id": "ex0vTJ-hY6Fj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "딥러닝 기반 자연어 처리 모델의 출력 역시 확률입니다. 하지만 사람은 자연어 형태의 출력을 선호합니다. 그것이 이해하기 쉽기도 하고요. 따라서 출력된 확률을 후처리(post processing)해서 자연어 형태로 바꿔 줍니다."
      ],
      "metadata": {
        "id": "2s_mk00WY6H9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이처럼 자연어 처리 모델은 자연어를 입력받아 해당 입력이 특정 범주일 확률을 출력하고, 이 확률값을 적당히 후처리해서 자연어 형태로 가공해 반환합니다.  \n",
        "이 책에서 다루는 ***문서 분류(document classification)*** , ***문장 쌍 분류(sentence pair classification)*** , ***개체명 인식(named entity recognition)*** , ***질의응답(question answering)*** , ***문장 생성(sentence generation)*** 등의 과제가 모두  그렇습니다."
      ],
      "metadata": {
        "id": "X0z-qFDwY6KS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 딥러닝 모델의 학습\n",
        "---\n",
        "딥러닝 자연어 처리 모델을 만들려면 무엇을 해야 할까요? 우선 데이터부터 준비해야 합니다. 다음처럼 각 문장에 '감성'이라는 ***레이블(label)*** 을 달아 놓은 자료가 있어야 합니다. 이를 ***학습 데이터(training data)*** 라고 부릅니다.  \n",
        "\n",
        "그다음은 모델이 데이터의 ***패턴(pattern)*** 을 스스로 익히게 해야 합니다. 이런 과정을 ***학습(train)*** 이라고 합니다. \n",
        "\n",
        "(그림 자료들)"
      ],
      "metadata": {
        "id": "3eVrsjG0Y6MW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <1-2 트랜스퍼 러닝>  \n",
        "\b이 책에서 소개하는 자연어 처리 모델의 학습 방법은 트랜스퍼 러닝입니다. 이 절에서는 프리트레인, 파인튜닝 등 트랜스퍼 러닝과 관련된 개념을 설명합니다."
      ],
      "metadata": {
        "id": "UM_mCmqgY6RA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 트랜스퍼 러닝\n",
        "---\n",
        "***트랜스퍼 러닝(transfer learning)*** 이란 특정 태스크를 학습한 모델을 다른 태스크 수행에 재사용하는 기법을 가리킵니다. 비유하자면 사람이 새로운 지식을 배울 때 그가 평생 쌓아왔던 지식을 요긴하게 다시 써먹는 것과 같습니다."
      ],
      "metadata": {
        "id": "7wjJSpjPc3fS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "트랜스퍼 러닝을 적용하면 기존보다 모델의 학습 속도가 빨라지고 새로운 태스크를 더 잘 수행하는 경향이 있습니다. 이 때문에 트랜스퍼 러닝은 최근 널리 쓰이고 있습니다. BERT나 GPT 등도 트랜스퍼러닝이 적용됐씁니다.  \n",
        "\n",
        "그림 1-6에서 태스크1은 ***업스트림(upstream)*** 태스크라고 부르고, 태스크2는 이와 대비된 개념으로 ***다운스트림(downstream)*** 태스크라고 부릅니다. 태스크1은 ***다음 단어 맞히기, 빈칸 채우기*** 등 대규모 말뭉치의 문맥을 이해하는 과제이며, 태스크2는 문서 분류, 개체명 인식 등 우리가 풀고자 하는 자연어 처리의 구체적인 문제들입니다.  \n",
        "\n",
        "업스트림 태스크를 학습하는 과정을 ***프리트레인(protrain)*** 이라고 부릅니다. 다운스트림 태스크를 본격적으로 수행하기에 앞서 학습(pretrain) 한다는 의미에서 이런 용어가 붙은 것으로 생각합니다."
      ],
      "metadata": {
        "id": "hZn0EGXUc3g3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 업스트림 태스크\n",
        "---\n",
        "트랜스퍼 러닝이 주목받게 된 것은 업스트림 태스크와 프리트레인 덕분입니다. 자연어의 풍부한 ***문맥(context)*** 을 모델에 내재화하고 이 모델을 다양한 다운스트림 태스크에 적용해 성능을 대폭 끌어올리게 된 것이죠.  \n",
        "\n",
        "대표적인 업스트림 태스크 가운데 하나가 ***다음 단어 맞히기*** 입니다. GPT 계열 모델이 바로 이 태스크로 프리트레인을 수행합니다. 예를 들어 다음 그림처럼 ***티끌 모아*** 라는 문맥이 주어졌고 학습 데이터 말뭉치에 ***티끌 모아 태산*** 이라는 구(phrase)가 많다고 하면 모델은 이를 바탕으로 다음에 올 단어를 ***태산*** 으로 분류하도록 학습됩니다."
      ],
      "metadata": {
        "id": "GFUV1jn8c3Ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델이 대규모 말뭉치를 가지고 이런 과정을 반복 수행하면 이전 문맥을 고려했을 때 어떤 단어가 그다음에 오는 것이 자연스러운지 알 수 있게 됩니다. 다시 말해 해당 언어의 풍부한 문맥을 이해할 수 있게 되는 것이죠. 이처럼 다음 단어 맞히기로 업스트림 태스크를 수행한 모델을 ***언어 모델(language model)*** 이라고 합니다."
      ],
      "metadata": {
        "id": "E6Qm2xCRc3Hy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "언어 모델을 학습하는 것은 앞서 언급한 감성 분석 모델의 학습 과정과 별반 다르지 않습니다. 감성 분석 예에서는 분류해야 할 범주의 수가 긍정, 부정으로 3개뿐이었지만, 언어 모델에서는 학습 대상 언어의 어휘 수(보통 수만 개 이상)만큼 늘어납니다. 예를 들어 ***티끌 모아*** 다음 단어의 정답이 ***태산*** 이라면 ***태산*** 이라는 단어에 해당하는 확률은 높이고, 나머지 단어들의 확률은 낮추는 방향으로 모델 전체를 업데이트합니다."
      ],
      "metadata": {
        "id": "pdSFWck5c3KP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "또 다른 업스트림 태스크로는 ***빈칸 채우기*** 가 있습니다. BERT 계열 모델이 바로 이 태스크로 프리트레인을 수행합니다. 다음 그림처럼 문장에서 빈칸을 만들고 해당 위치에 들어갈 단어가 무엇일지 맞히는 과정에서 학습됩니다."
      ],
      "metadata": {
        "id": "ZP-S6BxVY6m8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델이 많은 양의 데이터를 가지고 빈칸 채우기를 반복 학습하면 앞뒤 문맥을 보고 빈칸에 적합한 단어를 알 수 있습니다. 이 태스크를 수행한 모델 역시 언어 모델과 마찬가지로 해당 언어의 풍부한 문맥을 내재화할 수 있습니다. 이처럼 빈칸 채우기로 업스트림 태스크를 수행한 모델을 ***마스크 언어 모델(masked language model)*** 이라고 합니다.  \n",
        "\n",
        "마스크 언어 모델의 학습 역시 언어 모델과 비슷합니다. 그림 1-9에서 빈칸의 정답이 ***모아*** 라면 ***모아*** 라는 단어에 해당하는 확률은 높이고 나머지 단어와 관계된 확률은 낮추는 방향으로 모델 전체를 업데이트합니다."
      ],
      "metadata": {
        "id": "yd7gWpEBePu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞서 설펴본 감성 분석 모델 학습 예시에서 학습 데이터는 사람이 일일이 정답(레이블)을 만들어 줘야 했습니다. 이처럼 사람이 만든 정답 데이터로 모델을 학습하는 방법을 ***지도 학습(supervised learning) 이라고 합니다. 이 방식은 데이터를 만드는 데 비용이 많이 들뿐만 아니라 사람이 실수로 잘못된 레이블을 줄 수도 있습니다.  \n",
        "\n",
        "이에 반해 다음 단어 맞히기, 빈칸 채우기 같은 업스트림 태스크는 강력한 힘을 지닙니다. 뉴스, 웹 문서, 백과사전 등 글만 있으면 수작업 없이도 다량의 학습 데이터를 아주 싼값에 만들어 낼 수 있습니다. 덕분에 업스트림 태스크를 수행한 모델은 성능이 기존보다 월등히 좋아졌습니다.  \n",
        "이처럼 데이터 내에서 정답을 만들고 이를 바탕으로 모델을 학습하는 방법을 ***자기지도 학습(self-supervised learning)*** 이라고 합니다."
      ],
      "metadata": {
        "id": "ocWeyKRFhXxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 다운스트림 태스크\n",
        "---  \n",
        "\n",
        "우리가 모델을 업스트림 태스크로 프리트레인한 근본 이유는 다운스트림 태스크를 잘하기 위해서입니다."
      ],
      "metadata": {
        "id": "JbeOb07yhXzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞에서 설명했듯이 다운스트림 태스크는 우리가 풀어야 할 자연어 처리의 구체적인 과제들입니다. 보통 다운스트림 태스크는 프리트레인을 마친 모델을 구조 변경 없이 그대로 사용하거나 여기에 태스크 모듈을 덧붙인 형태로 수행합니다.  \n",
        "\n",
        "이 책에서 소개하는 다운스트림 태스크의 본질은 ***분류(classification)*** 입니다. 다시 말해 자연어를 입력받아 해당 입력이 어떤 범주에 해당하는지 확률 형태로 반환합니다. 문장 생성을 제외한 대부분의 과제에서는 프리트레인을 마친 마스크 언어 모델(BERT 계열)을 사용합니다.  \n"
      ],
      "metadata": {
        "id": "lPxNdsgohX15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 책에서 설명하는 다운스트림 태스크의 학습 방식은 모두 ***파인튜닝(fine-tuning)*** 입니다. 파인튜닝은 프리트레인을 마친 모델을 다운스트림 태스크에 맞게 업데이트하는 기법입니다. 예를 들어 문서 분류를 수행할 경우 프리트레인을 마친 BERT 모델 전체를 문서 분류 데이터로 업데이트합니다. 마찬가지로 개체명 인식을 수행한다면 BERT 모델 전체를 해당 데이터로 업데이트합니다."
      ],
      "metadata": {
        "id": "dv9IOVwyhX4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문서 분류\n",
        "---  \n",
        "문서 분류 모델은 자연어(문서나 문장)을 입력받아 해당 입력이 어떤 범주(긍정, 중립, 부정 따위)에 속하는지 그 확률값을 반환합니다.  "
      ],
      "metadata": {
        "id": "AeJjtX2qhX6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "구체적으로는 프리트레인을 마친 마스크 언어 모델 위에 작은 모듈을 하나 더 쌓아 문서 전체의 범주를 분류합니다. 문서 분류 과제는 4장에서 실습합니다. 한편 그림에서 CLS, SEP는 각각 문장의 시작과 끝에 붙이는 특수한 ***토큰(token)*** 입니다. 토큰 및 ***토큰화(tokenization)*** 에 관한 자세한 내용은 2장에서 다룹니다."
      ],
      "metadata": {
        "id": "qZn_k-rghX8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 자연어 추론\n",
        "---  \n",
        "자연어 추론 모델은 문장 2개를 입력받아 두 문장 사이의 관계가 참(entailment), 거짓(contradiction), 중립(neutral) 등 어떤 범주인지 그 확률값을 반환합니다.  \n"
      ],
      "metadata": {
        "id": "9n1rTMXKk6RN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "구체적으로는 프리트레인을 마친 마스크 언어 모델 위에 작은 모듈을 하나 더 쌓아 두 문장의 관계 범주를 분류합니다. 자연어 추론 과제는 5장에서 실습합니다."
      ],
      "metadata": {
        "id": "Mv3TRGUkk6Tt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 개체명 인식\n",
        "---\n",
        "개체명 인식 모델은 자연어(문서나 문장)를 입력받아 단어별로 기관명, 인명, 지명 등 어떤 개체명 범주에 속하는지 그 확률값을 반환합니다."
      ],
      "metadata": {
        "id": "q4wNY0Jbk6WD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "구체적으로는 프리트레인을 마친 마스크 언어 모델 위에 단어별로 작은 모듈을 쌓아 단어 각각의 개체명 범주를 분류합니다. 개체명 인식 과제는 6장에서 실습합니다."
      ],
      "metadata": {
        "id": "6tx18foDk6Zt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 질의응답\n",
        "---\n",
        "질의응답 모델은 자연어(질문+지문)를 입력받아 각 단어가 정답의 시작일 확률값과 끝일 확률값을 반환합니다."
      ],
      "metadata": {
        "id": "szbNcW-rk6b9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "구체적으로는 프리트레인을 마친 마스크 언어 모델 위에 단어별로 작은 모듈을 쌓아 전체 단어 가운데 어떤 단어가 시작인지 끝인지 분류합니다. 질의응답 과제는 7장에서 실습합니다."
      ],
      "metadata": {
        "id": "ApYDFTSsmYC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문장 생성\n",
        "---\n",
        "문장 생성 모델은 GPT 계열 언어 모델이 널리 쓰입니다. 문장 생성 모델은 자연어(문장)를 입력받아 어휘 전체에 대한 확률값을 반환합니다. 이 확률값은 입력된 문장 다음에 올 단어로 얼마나 적절한지를 나타내는 점수입니다."
      ],
      "metadata": {
        "id": "OG6OgR0_mYFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "구체적으로는 프리트레인을 마친 언어 모델을 구조 변경 없이 그대로 사용해, 문맥에 이어지는 적절한 다음 단어를 분류하는 방식입니다. 문장 생성 과제는 8장에서 실습합니다."
      ],
      "metadata": {
        "id": "AkAYZ5K2mYHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <파인튜닝 이외에 다운스트림 태스크를 학습하는 방법>  \n",
        "\n",
        ">- 파인튜닝(fine-tuning)\n",
        ">   - 다운스트림 태스크 데이터 전체를 사용합니다. 다운스트림 데이터에 맞게 모델 전체를 >업데이트합니다.\n",
        ">\n",
        ">- 프롬프트 튜닝(prompt tuning)\n",
        ">   - 다운스트림 태스크 데이터 전체를 사용합니다. 다운스트림 데이터에 맞게 모델 일부만 >업데이트합니다.\n",
        ">\n",
        ">- 인컨텍스트 튜닝(in-context tuning)\n",
        ">   - 다운스트림 태스크 데이터 일부만 사용합니다. 모델을 업데이트하지 않습니다.  \n",
        "\n"
      ],
      "metadata": {
        "id": "VgTJnzTXnFg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "파인튜닝 이외의 방식이 주목받고 있는 이유는 비용과 성능 때문입니다. 최근 언어 모델의 크기가 기하급수로 커지고 있는데요. 파인튜닝 방식으로 모델 전체를 업데이트하려면 많은 비용이 듭니다. 그 뿐만 아니라 프롬프트 튜닝, 인컨텍스트 러닝으로 학습한 몯델이 경쟁력 있는 태스크 수행 성능을 보일 때가 많습니다.\n",
        "\n",
        "인컨텍스트 러닝에는 다음 3가지 방식이 있습니다. 다운스트림 태스크 데이터를 몇 건 참고하느냐의 차이가 있을 뿐 모두 모델을 업데이트하지 않는다는 공통점이 있습니다. 모델을 업데이트하지 않고도 다운스트림 테스크를 바로 수행할 수 있다는 건 상당히 매력적입니다."
      ],
      "metadata": {
        "id": "5Rnn1Jj2oWz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">- 제로샷 러닝(zero-shot learning)\n",
        ">   - 다운스트림 태스크를 전혀 사용하지 않습니다. 모델이 바로 다운스트림 태스크를 수행합니다.  \n",
        ">\n",
        ">- 원샷 러닝(one-shot learning)\n",
        ">   - 다운스트림 태스크 데이터를 1건만 사용합니다. 모델은 1건의 데이터가 어떻게 수행되는지 참고한 >뒤 다운스트림 테스크를 수행합니다.\n",
        ">\n",
        ">- 퓨샷 러닝(few-shot learning)\n",
        ">   - 다운스트림 테스크 데이터를 몇 건만 사용합니다. 모델은 몇 건의 데이터가 어떻게 수행되는지 >참고한 뒤 다운스트림 태스크를 수행합니다."
      ],
      "metadata": {
        "id": "XSPV32ebo_fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4nbGQBkceG9C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}