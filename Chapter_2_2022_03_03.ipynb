{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_2_2022_03_02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxtYx16H3fN75uIIK2Nq7s"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## <2-4 토큰화하기>  \n",
        "\n",
        "이번에는 문장을 토큰화하고 해당 토큰들을 모델의 입력으로 만드는 과정을 실습해 보겠습니다."
      ],
      "metadata": {
        "id": "-e2enHxJQfL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT, BERT 입력값 만들기\n",
        "---  \n",
        "**[1단계]** 코랩 노트북 초기화 하기  \n",
        "\n",
        "이전 '어휘 집합 구축하기'와 마찬가지로 코랩에서 **'내 드라이브에 복사'** 와 **'하드웨어 가속기 사용 안 함(None)'** 으로 설정합니다. 노트북 초기화를 마치면 다음 코드를 실행해 의존성 있는 패키지를 설치합니다."
      ],
      "metadata": {
        "id": "hXrj3mXhQ1Iq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHgR-qIjQWSS",
        "outputId": "0883462f-4910-4381-e6ff-31ca797a2ed1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ratsnlp in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: flask-ngrok>=0.0.25 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (0.0.25)\n",
            "Requirement already satisfied: pytorch-lightning==1.3.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.3.4)\n",
            "Requirement already satisfied: flask-cors>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (3.0.10)\n",
            "Requirement already satisfied: flask>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4)\n",
            "Requirement already satisfied: transformers==4.10.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (4.10.0)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111)\n",
            "Requirement already satisfied: Korpora>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (0.2.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (2022.2.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (4.62.3)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (0.18.2)\n",
            "Requirement already satisfied: torchmetrics>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (0.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (21.3)\n",
            "Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (1.21.5)\n",
            "Requirement already satisfied: PyYAML<=5.4.1,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (5.4.1)\n",
            "Requirement already satisfied: pyDeprecate==0.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (0.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (0.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (0.0.47)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (4.11.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (2019.12.20)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (2.11.3)\n",
            "Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors>=3.0.10->ratsnlp) (1.15.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers==4.10.0->ratsnlp) (3.10.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=1.1.4->ratsnlp) (2.0.1)\n",
            "Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.7/dist-packages (from Korpora>=0.2.0->ratsnlp) (0.6)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from Korpora>=0.2.0->ratsnlp) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch-lightning==1.3.4->ratsnlp) (3.0.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (3.0.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (1.44.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (3.3.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.10.0->ratsnlp) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (3.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (1.7.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (0.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (21.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (1.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0->ratsnlp) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ratsnlp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "그리고 이전 실습에서 미리 구축해 놓은 어휘 집합을 구글 드라이브에 저장해 두었으므로 다음 코드를 실행해 자신의 구글 드라이브를 코랩 노트북과 연결합니다."
      ],
      "metadata": {
        "id": "FCOLrbWPRiy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg3bhT86RhTN",
        "outputId": "1bdd0c51-3660-42cd-a4b8-cf9adc01d932"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[2단계]** GPT 입력값 만들기  \n",
        "GPT 입력값을 만들려면 토크나이저부터 준비해야 합니다. 다음 코드를 수행하면 GPT 모델이 사용하는 토크나이저를 초기화할 수 있습니다. 먼저 자슨의 구글 드라이브에는 이전 실습에서 만든 바이트 기준 BPE 어휘 집합(vocab.json)과 바이그램 쌍의 병합 우선순위(merge.txt)가 있어야 합니다."
      ],
      "metadata": {
        "id": "LzbsA10yRi1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GPT 토크나이저 선언\n",
        "from transformers import GPT2Tokenizer\n",
        "tokenizer_gpt = GPT2Tokenizer.from_pretrained(\"/gdrive/My drive/nlpbook/bbpe\")\n",
        "tokenizer_gpt.pad_token = \"[PAD]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "-5Qhkty_TXrq",
        "outputId": "8d049a19-68ed-4d7f-bfe1-9d1151357f6e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b8142c312096>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#GPT 토크나이저 선언\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer_gpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/gdrive/My drive/nlpbook/bbpe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer_gpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"[PAD]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1651\u001b[0m                 \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m                 \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m             )\n\u001b[1;32m   1655\u001b[0m             additional_files_names = {\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mget_fast_tokenizer_file\u001b[0;34m(path_or_repo, revision, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   3424\u001b[0m     \u001b[0;31m# Inspect all files from the repo/folder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3425\u001b[0m     all_files = get_list_of_files(\n\u001b[0;32m-> 3426\u001b[0;31m         \u001b[0mpath_or_repo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3427\u001b[0m     )\n\u001b[1;32m   3428\u001b[0m     \u001b[0mtokenizer_files_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_list_of_files\u001b[0;34m(path_or_repo, revision, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1729\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1730\u001b[0m     model_info = HfApi(endpoint=HUGGINGFACE_CO_RESOLVE_ENDPOINT).model_info(\n\u001b[0;32m-> 1731\u001b[0;31m         \u001b[0mpath_or_repo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1732\u001b[0m     )\n\u001b[1;32m   1733\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfilename\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msiblings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mmodel_info\u001b[0;34m(self, repo_id, revision, token, timeout)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"authorization\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"Bearer {token}\"\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m         \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mModelInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/api/models//gdrive/My%20drive/nlpbook/bbpe"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 위 url 오류는 해결하는 대로 다시 커밋하도록 하겠습니다! 죄송합니다."
      ],
      "metadata": {
        "id": "V5Cx76RORP9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음 코드는 예시 문장 3개를 바이트 수준 BPE 토크나이저로 토큰화합니다."
      ],
      "metadata": {
        "id": "2p9MPo7ERi4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GPT 토크나이저로 토큰화하기\n",
        "sentences = [\n",
        "    \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
        "    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n",
        "    \"별루 였다..\",\n",
        "]\n",
        "tokenized_sentences = [tokenizer_gpt.tokenize(sentence) for sentence in sentences]"
      ],
      "metadata": {
        "id": "tNPZoTy9ZKtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드를 실행한 결과를 보고자 코랩에서 tokenized_sentences를 출력해 보면 다음과 같습니다. 그런데 출력 결과를 보면 토큰들이 알 수 없는 문자열로 구성돼 있음을 확인할 수 있습니다. 그 이유는 앞에서도 설명했듯이 GPT 모델은 바이트 기준 BPE를 적용하기 때문입니다."
      ],
      "metadata": {
        "id": "0OyPoTIxZsY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞 코드는 GPT 토크나이저의 토큰화 결과를 살짝 맛보려고 한 것이고, 실제 모델 입력값은 다음 코드로 만듭니다."
      ],
      "metadata": {
        "id": "msovm-B5Ri6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GPT 모델 입력 만들기\n",
        "batch_inputs = tokenizer_gpt(\n",
        "    sentences,\n",
        "    padding=\"max_length\",  # 문장의 최대 길이에 맞춰 패딩\n",
        "    max_length=12,  # 문장의 토큰 기준 최대 길이\n",
        "    truncation=True,  # 문장 잘림 허용 옵션\n",
        ")"
      ],
      "metadata": {
        "id": "hDfMpaziaWUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드의 실행 결과로 2가지 입력값이 만들어집니다. 하나는 input_ids입니다. batch_inputs[\"input_ids\"]를 코랩에서 실행해 그 결과를 출력해 보면 다음 표와 같습니다. input_ids는 토큰화 결과를 가지고 각 토큰을 인덱스로 바꾼 것입니다. 어휘 집합(vocap.json)을 확인해 보면 각 어휘 순서대로 나열된 것을 확인할 수 있는데요, 이 순서가 바로 인덱싱입니다. 이처럼 각 토큰을 인덱스로 변환화는 과정을 **인덱싱(indexing)**이라고 합니다."
      ],
      "metadata": {
        "id": "45iUU90LRi9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><표 2-9 GPT의 input_ids></center>  \n",
        "\n",
        "|**구분**|**토큰 1**|**토큰 2**|**토큰 3**|**토큰 4**|**토큰 5**|**토큰 6**|**토큰 7**|**토큰 8**|**토큰 9**|**토큰 10**|**토큰 11**|**토큰 12**|      \n",
        "|------|------|------|------|------|------|------|------|------|------|------|------|------|\n",
        "|문장1|334|2338|263|581|4055|464|3808|0|0|0|0|0|0|\n",
        "|문장2|3693|336|2876|758|2883|356|806|422|9875|875|2960|7292|\n",
        "|문장3|4957|451|3653|263|0|0|0|0|0|0|0|0|0|"
      ],
      "metadata": {
        "id": "IESFxvC6Ri_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "표를 자세히 보면 모든 문장의 길이(토큰 수)가 12로 맞춰진 것을 볼 수 있습니다. 위 코드에서 max_length 인자에 12를 넣었기 때문인데요, 이보다 짧은 문장1과 문장3은 뒤에 [PAD]토큰에 해당하는 인덱스 0이 붙었습니다. [PAD]토큰은 일종의 더미 토큰으로 길이를 맞춰주는 역할을 합니다. 문장2는 원래 토큰 길이가 15였는데 12로 줄었습니다. 코드에서 문장 잘림을 허용하는 truncation=True 옵션 때문입니다. "
      ],
      "metadata": {
        "id": "CY6S3O_CiIzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "또한 위 코드의 실행 결과로 attention_mask도 만들어집니다. attention_mask는 일반 토큰이 자리한 곳(1)과 패딩 토큰이 자리한 곳(0)을 구분해 알려주는 장치입니다. batch_inputs[\"attention_mask\"]를 입력해 그 결과를 출력해 보면 다음 표와 같습니다."
      ],
      "metadata": {
        "id": "5UBfBA9ZiKDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><표 2-10 GPT의 attention_mask></center>  \n",
        "\n",
        "|**구분**|**토큰 1**|**토큰 2**|**토큰 3**|**토큰 4**|**토큰 5**|**토큰 6**|**토큰 7**|**토큰 8**|**토큰 9**|**토큰 10**|**토큰 11**|**토큰 12**|      \n",
        "|------|------|------|------|------|------|------|------|------|------|------|------|------|\n",
        "|문장1|1|1|1|1|1|1|1|0|0|0|0|0|0|\n",
        "|문장2|1|1|1|1|1|1|1|1|1|1|1|1|1|\n",
        "|문장3|1|1|1|1|0|0|0|0|0|0|0|0|0|"
      ],
      "metadata": {
        "id": "h3_lfFVnevlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[3단계]** BERT 입력값 만들기  \n",
        "이번엔 BERT 모델의 입력값을 만들어 보겠습니다. 다음 코드를 수행하면 BERT 모델이 사용하는 토크나이저를 초기화할 수 있습니. 먼저 자신의 구글 드라이브 경로에는 이전 실습에서 만들 BERT용 워드피스 어휘 집합(vocab.txt)이 있어야 합니다."
      ],
      "metadata": {
        "id": "4IJADw-rRjCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT 토크나이저 선언\n",
        "from transformers import BertTokenizer\n",
        "tokenizer_bert = BertTokenizer.from_pretrained(\n",
        "    \"gdrive/My drive/nlpbook/wordpiece\",\n",
        "    do_lower_case=False,\n",
        ")"
      ],
      "metadata": {
        "id": "uvePMIqZeU_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음 코드는 예시 문장 3개를 워드피스 토크나이저로 토큰화합니다. 토큰 일부에 있는 **##**은 해당 토큰이 어절(띄어쓰기 기준)의 시작이 아님을 나타냅니다. 예를 들어 **##네요**는 이 토큰이 앞선 토큰 **짜증나**와 같은 어절에 위치하며 어절 내에서 연속됨을 표시합니다.  "
      ],
      "metadata": {
        "id": "zHCnxalgRjEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT 토크나이저로 토큰화하기\n",
        "sentences = [\n",
        "    \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
        "    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n",
        "    \"별루 였다..\",\n",
        "]\n",
        "tokenized_sentences = [tokenizer_bert.tokenize(sentence) for sentence in sentences]"
      ],
      "metadata": {
        "id": "1WYZ0d_igQVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞 코드는 워드피스 토크나이저의 토큰화 결과를 살짝 맛보려고 한 것이고, 실제 모델 입력값은 다음 코드로 만듭니다."
      ],
      "metadata": {
        "id": "izHcEXXkevjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT 모델 입력 만들기\n",
        "batch_inputs = tokenizer_bert(\n",
        "    sentences,\n",
        "    padding=\"max_length\",\n",
        "    max_length=12,\n",
        "    truncation=True,\n",
        ")"
      ],
      "metadata": {
        "id": "ERflJkpVkz4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "코드의 실행 결과로 세 가지 입력값이 만들어집니다. 하나는 GPT 모델과 마찬가지로 토큰 인덱스 시퀀스를 나타내는 input_ids입니다. batch_inputs[\"input_ids\"]를 입력하고 이를 출력해 보면 다음 표와 같습니다."
      ],
      "metadata": {
        "id": "Mk2VEof2evoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><표 2-11 BERT의 input_ids></center>  \n",
        "\n",
        "|**구분**|**토큰 1**|**토큰 2**|**토큰 3**|**토큰 4**|**토큰 5**|**토큰 6**|**토큰 7**|**토큰 8**|**토큰 9**|**토큰 10**|**토큰 11**|**토큰 12**|      \n",
        "|------|------|------|------|------|------|------|------|------|------|------|------|------|\n",
        "|문장1|2|621|2631|16|16|1993|3678|1990|3323|3|0|0|0|\n",
        "|문장2|2|997|16|16|16|2609|2045|2796|1981|1212|16|3|\n",
        "|문장3|2|3274|9507|16|16|3|0|0|0|0|0|0|0|"
      ],
      "metadata": {
        "id": "5bB9ufg2evqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "표를 자세히 보면 모든 문장 앞에 2, 끝에 3이 붙은 것을 확인할 수 있습니다. 이는 각각 [CLS], [SEP]라는 토큰에 대응하는 인덱스인데요, BERT는 문장 시작과 끝에 이 2개 토큰을 덧붙이는 특징이 있습니다. 그리고 attention_mask도 만들어집니다. BERT의 attention_mask는 GPT와 마찬가지로 일반 토큰이 자리한 곳(1)과 패딩 토큰이 자리한 곳(0)을 구분해서 알려줍니다."
      ],
      "metadata": {
        "id": "mOqyznV9evsi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><표 2-12 BERT의 attention_mask></center>  \n",
        "\n",
        "|**구분**|**토큰 1**|**토큰 2**|**토큰 3**|**토큰 4**|**토큰 5**|**토큰 6**|**토큰 7**|**토큰 8**|**토큰 9**|**토큰 10**|**토큰 11**|**토큰 12**|      \n",
        "|------|------|------|------|------|------|------|------|------|------|------|------|------|\n",
        "|문장1|1|1|1|1|1|1|1|1|1|1|0|0|0|\n",
        "|문장2|1|1|1|1|1|1|1|1|1|1|1|1|1|\n",
        "|문장3|1|1|1|1|1|1|0|0|0|0|0|0|0|"
      ],
      "metadata": {
        "id": "b5Jfqqx4nWXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "마지막으로 **token_type_ids**라는 입력값도 만들어집니다. 이는 세그먼트(segment)에 해당하는 것으로 모두 0입니다. 세그먼트 정보를 입력하는 건 BERT 모델의 특징입니다. BERT 모델은 기본적으로 문서(혹은 문장) 2개를 입력받는데요, 둘은 **token_type_ids**로 구분합니다. 첫 번째 세그먼트(문서 혹은 문장)에 해당하는 **token_type_ids**는 0, 두 번째 세그먼트는 1입니다.  \n",
        "이번 실습에서 우리는 문장을 하나씩 넣었으므로 **token_type_ids**가 모두 0으로 처리됩니다."
      ],
      "metadata": {
        "id": "syVDTxkqnWZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <알아두면 좋아요!>  \n",
        "\n",
        "토큰화와 관련해 가장 좋은 학습 자료는 허깅페이스 토크나이저 공식 문서입니다. 허깅페이스 토크나이저는 바이트 페어 인코딩, 워드피스 등 각종 서브워드 토큰화 기법은 물론 유니코드 정규화, 프리토크나이저, 토큰화 후처리 등 다양한 기능을 제공합니다. 공식 문서(영어)가 꽤 상세해서 학습 자료로 손색이 없습니다.  \n",
        "\n",
        "- www.huggingface.co/docs/tokenizers/python/latest\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RQrlhOtipJ_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6CXY8qZknVxi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}