{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_3_2022_03_16.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPw6HdmUVce6+tTb+HGOs2h"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## <3-5 BERT와 GPT 비교> "
      ],
      "metadata": {
        "id": "xvEFIRnCqIYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번 절에서는 트랜스포머 아키텍처를 기본 뼈대로 하는 BERT와 GPT 모델의 공통점과 차이점을 중심으로 살펴봅니다.   \n",
        "마지막으로는 트랜스포머 계열 언어 모델의 최근 경향도 설명합니다."
      ],
      "metadata": {
        "id": "_ALLHPjcqI7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT와 GPT  \n",
        "---\n",
        "GPT는 언어 모델입니다. 이전 단어들이 주어졌을 때 다음 단어가 무엇인지 맞히는 과정에서 프리트레인합니다. 문장 왼쪽부터 오른쪽으로 순차적으로 계산한다는 점에서 일방향(unidirectional)입니다.  \n",
        "BERT는 마스크 언어 모델입니다. 문장 중간에 빈칸을 만들고 해당 빈칸에 어떤 단어가 적절할지 맞히는 과정에서 프리트레인합니다. 빈칸 앞뒤 문맥을 모두 살필 수 있다는 점에서 양방향(bidirectional) 성격을 가집니다.  \n"
      ],
      "metadata": {
        "id": "EwoyNky3qI94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 때문에 GPT는 문장 생성에, BERT는 문장의 의미를 추출하는 데 강점을 지닌 것으로 알려져 있습니다.  \n",
        "다음 그림은 GPT와 BERT의 프리트레인 방식을 나타낸 것입니다. "
      ],
      "metadata": {
        "id": "QMd-7AGTqJAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><그림 3-54 GPT와 BERT의 프리트레인 방법></center>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://i.imgur.com/S0equuG.png\">  \n",
        "\n",
        "<center>출처 : ratsgo's NLPBOOK</center>\n"
      ],
      "metadata": {
        "id": "F07MRlGuqJC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "한편 트랜스포머에서 BERT는 인코더, GPT는 디코더만 취해 사용한다는 것 역시 다른 점입니다.  \n",
        "구조상 차이는 이어서 살펴보겠습니다."
      ],
      "metadata": {
        "id": "ewgCPflPqJFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT 구조\n",
        "---\n",
        "다음 그림은 GPT의 구조를 나타낸 것입니다. GPT는 트랜스포머에서 인코더를 제외하고 디코더만 사용합니다.  \n",
        "그림에서 오른쪽 디코더 블록을 자세히 보면 인코더 쪽에서 보내오는 정보를 받는 모듈(멀티 헤드 어텐션) 역시 제거돼 있음을 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "qnGsWdMiqJHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><그림 3-55 GPT 구조></center>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://i.imgur.com/Q7IS78n.png\">  \n",
        "\n",
        "<center>출처 : ratsgo's NLPBOOK</center>"
      ],
      "metadata": {
        "id": "P4yV2rQYqJKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음 그림은 GPT의 셀프 어텐션을 나타낸 것입니다.  \n",
        "입력 단어 시퀀스가 **어제 카페 갔었어 거기 사람 많더라**이고 이번이 **카페**를 맞혀야 하는 상황이라고 가정해 보겠습니다. 이때 GPT는 정답 단어 **카페**를 맞힐 때 **어제**라는 단어만 참고할 수 있습니다."
      ],
      "metadata": {
        "id": "4p85r2haqJM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><그림 3-56 GPT의 셀프 어텐션 (1)></center>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://i.imgur.com/oPXpfWk.jpg\">  \n",
        "\n",
        "<center>출처 : ratsgo's NLPBOOK</center>"
      ],
      "metadata": {
        "id": "6dRnEWN7qJO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "따라서 정답 단어 이후의 모든 단어(**카페~많더라**)를 볼 수 없도록 처리해 줍니다. 구체적으로는 밸류 벡터들을 가중합할 때 참고할 수 없는 단어에 곱하는 점수가 0이 되도록 합니다.  \n",
        "이와 관련한 내용은 '3-3'절을 참고하기 바랍니다!"
      ],
      "metadata": {
        "id": "nb6DK_ohqJRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**어제**라는 단어에 대응하는 GPT 마지막 레이어의 출력 결과에 선형 변환과 소프트맥스를 적용해 요솟값 각각이 확률이고 학습 대상 언어의 어휘 수만큼 차원 수를 가진 벡터가 되도록 합니다. 그리고 이번 차례의 정답인 **카페**에 해당하는 확률은 높이고, 나머지 단어의 확률은 낮아지도록 모델 전체를 업데이트합니다."
      ],
      "metadata": {
        "id": "085Er7qms4fa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><그림 3-57 GPT의 학습 (1)></center>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://i.imgur.com/mnzuvVg.jpg\">  \n",
        "\n",
        "<center>출처 : ratsgo's NLPBOOK</center>"
      ],
      "metadata": {
        "id": "2l-zRpSUs4hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번에는 **갔었어**를 맞혀야 하는 상황입니다.  \n",
        "이때 GPT는 정답 단어 **갔었어**를 맞힐 때 **어제**와 **카페**라는 단어를 참고할 수 있습니다."
      ],
      "metadata": {
        "id": "DCbO9VEfs4kC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><그림 3-58 GPT의 셀프 어텐션 (2)></center>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://i.imgur.com/1R0Cbbk.jpg\">  \n",
        "\n",
        "<center>출처 : ratsgo's NLPBOOK</center>"
      ],
      "metadata": {
        "id": "gKn-L0mPs4mq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**카페**라는 단어에 대응하는 GPT 마지막 레이어의 출력 결과에 선형 변환과 소프트맥스를 적용해 요솟값 각각이 확률이고 학습 대상 언어의 어휘 수만큼 차원 수를 가진 벡터가 되도록 합니다. 그리고 이번 차례의 정답인 **갔었어**에 해당하는 확률은 높이고 나머지 단어의 확률은 낮아지도록 모델 전체를 업데이트합니다."
      ],
      "metadata": {
        "id": "zEnPH4Bds4pa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><그림 3-57 GPT의 학습 (2)></center>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://i.imgur.com/3h5Y3TX.jpg\">  \n",
        "\n",
        "<center>출처 : ratsgo's NLPBOOK</center>"
      ],
      "metadata": {
        "id": "KNQRFOG3uVgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**거기**를 맞혀야 하는 상황이라면 모델은 **어제, 카페, 갔었어** 세 단어를 참고할 수 있습니다."
      ],
      "metadata": {
        "id": "eocueAMZuVid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><그림 3-60 GPT의 셀프 어텐션 (3)></center>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://i.imgur.com/wXGfHBQ.jpg\">  \n",
        "\n",
        "<center>출처 : ratsgo's NLPBOOK</center>"
      ],
      "metadata": {
        "id": "0ElOTjuIuVkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**갔었어**라는 단어에 대응하는 GPT 마지막 레이어의 출력 결과에 선형 변환과 소프트맥스를 적용해 요솟값 각각이 확률이고 학습 대상 언어의 어휘 수만큼 차원 수를 가진 벡터가 되도록 합니다.  \n",
        "이번 차례의 정답인 **거기**에 해당하는 확률은 높이고 나머지 단어의 확률은 낮아지도록 모델 전체를 업데이트합니다."
      ],
      "metadata": {
        "id": "U4s6_Drsu8ZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><그림 3-61 GPT의 학습 (3)></center>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://i.imgur.com/79lXt7h.jpg\">  \n",
        "\n",
        "<center>출처 : ratsgo's NLPBOOK</center>"
      ],
      "metadata": {
        "id": "s6LQEL2xu8bc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT 구조\n",
        "---\n",
        "그림 3-62는 BERT의 구조를 나타낸 것입니다. BERT는 트랜스포머에서 디코더를 제외하고 인코더만 사용합니다."
      ],
      "metadata": {
        "id": "qgyMZJvgqJSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><그림 3-62 BERT 구조></center>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://i.imgur.com/ekmm63h.png\">  \n",
        "\n",
        "<center>출처 : ratsgo's NLPBOOK</center>"
      ],
      "metadata": {
        "id": "e6iSlEO8vdyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "그림 3-63은 BERT의 셀프 어텐션을 나타낸 것입니다.  \n",
        "입력 단어 시퀀스가 **어제 카페 갔었어 (MASK) 사람 많더라**라고 가정해 보겠습니다. 그림에서 확인할 수 있듯이 BERT는 마스크 토큰 앞뒤 문맥을 모두 참고할 수 있습니다. 앞뒤 정보를 준다고 해서 정답을 미리 알려주는 것이 아니기 때문입니다."
      ],
      "metadata": {
        "id": "qMIEgYEkvd0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><그림 3-63 BERT의 셀프 어텐션></center>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://i.imgur.com/pdBIXTT.jpg\">  \n",
        "\n",
        "<center>출처 : ratsgo's NLPBOOK</center>"
      ],
      "metadata": {
        "id": "4Fz0Vm3Qv1fR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(MASK)**라는 단어에 대응하는 BERT 마지막 레이어의 출력 결과에 선형 변환과 소프트맥스를 적용해 요솟값 각각이 확률이고 학습 대상 언어의 어휘 수만큼 차원 수를 가진 벡터가 되도록 합니다. 빈칸의 정답인 **거기**에 해당하는 확률은 높이고 나머지 단어의 확률은 낮아지도록 모델 전체를 업데이트합니다."
      ],
      "metadata": {
        "id": "F9PQisZ9vd2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><그림 3-64 BERT의 학습></center>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://i.imgur.com/79lXt7h.jpg\">  \n",
        "\n",
        "<center>출처 : ratsgo's NLPBOOK</center>"
      ],
      "metadata": {
        "id": "Gn0u7TEewZxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 최근 트렌드\n",
        "---\n",
        "트랜스포머 계열 언어 모델의 최근 트렌드를 잠시 살펴보겠습니다.  \n",
        "우선 모델 크기 증가 추세가 눈에 띕니다. GPT3가 대표적인데요, 표 3-1을 보면 파라미터 수 기준 GPT3는 GPT1보다 1,400배, GPT2보다 117배 증가했습니다. OpenAI에 따르면 모델 크기 증가는 언어 모델 품질은 물론 각종 다운스트림 task의 성능 개선에 큰 도움이 된다고 합니다."
      ],
      "metadata": {
        "id": "zxux4FZiwZzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><표 3-1 GPT 모델 크기 비교></center>\n",
        "\n",
        "|**모델명**|**크기**|                     \n",
        "|------|------|\n",
        "|GPT1|0.125B|\n",
        "|GPT2|1.5B|\n",
        "|GPT3|175B|\n"
      ],
      "metadata": {
        "id": "6B7BP7KowZ1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이와 별개로 모델 성능을 최대한 유지하면서 계산량 혹은 모델의 크기를 줄이려는 시도도 계속되고 있습니다. 디스틸레이션(distillation), 퀀타이제이션(quantization), 프루닝(pruning), 파라미터 공유(weight sharing) 등이 바로 그것입니다. 자연어 처리 분야에서 트랜스포머의 영향력은 한동안 지속될 것으로 보입니다."
      ],
      "metadata": {
        "id": "0SMEQcHBx5Sl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKbEy678qDtC"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}